# Trackable name for objects created from this Chart. 
appPrefix: pytorch-distributed

MultiprocessingDistributed:
  # N Nodes
  replicas: 2

  # N GPU per Node
  nproc_per_node: 1

  # Defined this way in the event of MIG labels but should equal nproc_per_node.
  resources:
    limits:
      nvidia.com/gpu: 1

  # Helpful for heterogenous machine environments.
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: nvidia.com/gpu.count
                operator: In
                values:
                  - "1"
                  - "8"

  # This is what actually runs in the container.
  args:
    - python
    - train.py
    - --model=resnet50
    - --batch-size=64
    - --epochs=60
    - --lr=0.1
    - --momentum=0.9
    - --weight-decay=0.00002
    - --lr-step-size=30
    - --lr-gamma=0.1
    - --print-freq=1
    - --output-dir=/workspace/data
    - --amp

# Default image is on Docker hub now but may be pushed/pulled elsewhere.
createCredentials: false
imageCredentials: {}
  # registry: nvcr.io
  # username: $oauthtoken
  # password: "api-key"
  # email: "user@nvidia.com"

# This should be what was defined in the Docker Compose.
imagePullSecrets: []
  # - container-secret
baseImage:
  repository: tuttlebr/distributed-pytorch
  pullPolicy: Always
  tag: "v0.1.0"
